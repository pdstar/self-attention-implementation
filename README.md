# self-attention-implementation
Implement the scaled multiplicative attention calculation used by the multi-head attention class used in GPT models
